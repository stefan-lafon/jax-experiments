## Distributed CIFAR-10 training in JAX and Flax
Distributed CIFAR-10 training in JAX and Flax, scaling from local JIT execution to hardware-accelerated SPMD. [Notebook](notebooks/01_distributed_cnn.ipynb)

![classification](assets/cifar.png)

## Jax's higher order derivatives and Newton's method
JAX makes it easy to take higher order derivatives. Using it to implement Newton's method. [Notebook](notebooks/02_curvature_and_newton_methods.ipynb)

![newton_vs_gradient_descent](assets/newton.png)