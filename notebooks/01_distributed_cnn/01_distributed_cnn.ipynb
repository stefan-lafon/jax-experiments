{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPFDWfOpvW57cL3JGF/w+g5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"J3xiVYz13vUI"}},{"cell_type":"markdown","source":["##Distributed CNN\n","JAX implementation of a CNN, its training loop and evaluation on CIFAR-10 dataset.\n","\n","Notes:\n","- we use categorical cross entropy\n","- also, we first train it serially (jit), then we use pmap parallelism."],"metadata":{"id":"HXCmFbCL2KFI"}},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RrHgyoCkyL_c"},"outputs":[],"source":["# @title Setup\n","import os\n","\n","# Must be set before JAX/XLA init to partition host CPU for pmap testing.\n","# Re-run after restarting the runtime if you need to change this.\n","os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=4'\n","\n","import jax\n","import jax.numpy as jnp\n","import numpy as np\n","import tensorflow as tf\n","\n","# Keep TF's hands off the GPU memory; JAX is the primary compute engine here.\n","tf.config.set_visible_devices([], 'GPU')\n","\n","def report_environment():\n","    backend = jax.default_backend()\n","    devices = jax.devices()\n","\n","    print(f\"JAX Backend: {backend.upper()}\")\n","    print(f\"Primary Devices: {len(devices)}\")\n","    for d in devices:\n","        print(f\" - {d.device_kind} (ID: {d.id})\")\n","\n","    if backend == 'gpu':\n","        print(\"\\nHardware Driver Status:\")\n","        # Direct check for driver/CUDA alignment\n","        try:\n","            !nvidia-smi --query-gpu=driver_version,compute_cap --format=csv,noheader\n","        except:\n","            print(\"nvidia-smi check failed.\")\n","\n","    print(f\"\\nSoftware Stack:\")\n","    print(f\" - JAX: {jax.__version__}\")\n","    print(f\" - Local Device Count: {jax.local_devices()}\")\n","\n","report_environment()"]},{"cell_type":"code","source":["# @title Data pipeline\n","import tensorflow_datasets as tfds\n","\n","def load_cifar10(batch_size, train=True):\n","    split = 'train' if train else 'test'\n","    ds, info = tfds.load('cifar10', split=split, with_info=True, as_supervised=True)\n","\n","    def preprocess(image, label):\n","        image = tf.cast(image, tf.float32) / 255.0\n","        label = tf.one_hot(label, 10)\n","        return image, label\n","\n","    ds = ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    if train:\n","        ds = ds.shuffle(10000).repeat()\n","\n","    ds = ds.batch(batch_size, drop_remainder=True)\n","    # Ensure the host stays ahead of the accelerator.\n","    ds = ds.prefetch(tf.data.AUTOTUNE)\n","\n","    # Use as_numpy to avoid TF tensor overhead in JAX.\n","    return tfds.as_numpy(ds), info\n","\n","# Initialize generators.\n","BATCH_SIZE = 64\n","train_ds_iterable, ds_info = load_cifar10(BATCH_SIZE, train=True)\n","test_ds_iterable, _ = load_cifar10(BATCH_SIZE, train=False)\n","\n","# Create iterators for manual stepping.\n","train_ds = iter(train_ds_iterable)\n","test_ds = iter(test_ds_iterable)\n","\n","# Verification.\n","sample_batch = next(train_ds)\n","print(f\"Batch shapes: Images {sample_batch[0].shape}, Labels {sample_batch[1].shape}\")\n","print(f\"Data types:  Images {sample_batch[0].dtype}, Labels {sample_batch[1].dtype}\")"],"metadata":{"cellView":"form","id":"l3SEor4v1wOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Model definition\n","import flax.linen as nn\n","\n","class CIFAR10CNN(nn.Module):\n","    @nn.compact\n","    def __call__(self, x):\n","        x = nn.Conv(features=32, kernel_size=(3, 3), padding='SAME')(x)\n","        x = nn.relu(x)\n","        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n","\n","        x = nn.Conv(features=64, kernel_size=(3, 3), padding='SAME')(x)\n","        x = nn.relu(x)\n","        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n","\n","        x = x.reshape((x.shape[0], -1))\n","        x = nn.Dense(features=256)(x)\n","        x = nn.relu(x)\n","        x = nn.Dense(features=10)(x)\n","        return x\n","\n","model = CIFAR10CNN()\n","key = jax.random.PRNGKey(0)\n","\n","# Initialize with a dummy input to fix parameter shapes.\n","variables = model.init(key, jnp.ones((1, 32, 32, 3)))\n","params = variables['params']\n","\n","# Quick shape verification.\n","jax.tree_util.tree_map(lambda x: print(f\"Layer shape: {x.shape}\"), params)"],"metadata":{"cellView":"form","id":"kYJxSdwk4o6i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Loss and update logic\n","\n","import optax\n","\n","# Standard Adam optimizer setup.\n","learning_rate = 1e-3\n","optimizer = optax.adam(learning_rate)\n","opt_state = optimizer.init(params)\n","\n","def compute_loss(params, images, labels):\n","    \"\"\"\n","    Computes cross-entropy loss between model logits and one-hot labels.\n","    \"\"\"\n","    logits = model.apply({'params': params}, images)\n","    # Using the current optax API for categorical cross entropy.\n","    loss = optax.softmax_cross_entropy(logits=logits, labels=labels)\n","    return jnp.mean(loss)\n","\n","@jax.jit\n","def train_step(params, opt_state, images, labels):\n","    \"\"\"\n","    Fuses the gradient computation and parameter update into one XLA kernel.\n","    \"\"\"\n","    # Calculate scalar loss and the gradients for the parameter Pytree.\n","    loss, grads = jax.value_and_grad(compute_loss)(params, images, labels)\n","\n","    # Transform gradients into updates based on the optimizer state.\n","    updates, opt_state = optimizer.update(grads, opt_state, params)\n","\n","    # Apply those updates to the current parameters.\n","    params = optax.apply_updates(params, updates)\n","\n","    return params, opt_state, loss\n","\n","# Single execution to trigger JIT compilation and verify the step.\n","params, opt_state, loss = train_step(params, opt_state, sample_batch[0], sample_batch[1])\n","print(f\"Initial loss: {loss:.4f}\")"],"metadata":{"cellView":"form","id":"c83HUnpcH1EM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Training loop and evaluation\n","\n","import time\n","\n","def train_model(params, opt_state, dataset, num_steps=500):\n","    \"\"\"\n","    Main training loop for CIFAR-10.\n","    \"\"\"\n","    step_losses = []\n","    start_time = time.time()\n","\n","    print(f\"Starting training for {num_steps} steps...\")\n","\n","    for step in range(num_steps):\n","        # Pull the next pre-processed batch from the iterator.\n","        batch_images, batch_labels = next(dataset)\n","\n","        # Execute the compiled train_step on the GPU.\n","        params, opt_state, loss = train_step(params, opt_state, batch_images, batch_labels)\n","        step_losses.append(loss)\n","\n","        if step % 100 == 0:\n","            avg_loss = np.mean(step_losses[-100:])\n","            print(f\"Step {step:4d} | Loss: {avg_loss:.4f}\")\n","\n","    end_time = time.time()\n","    total_time = end_time - start_time\n","\n","    print(f\"\\nTraining complete.\")\n","    print(f\"Total time: {total_time:.2f}s | Speed: {num_steps/total_time:.2f} steps/s\")\n","\n","    return params, opt_state, step_losses\n","\n","# Execute the training.\n","params, opt_state, history = train_model(params, opt_state, train_ds)"],"metadata":{"cellView":"form","id":"M2xdBBPIJHMe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Model evaluation\n","\n","@jax.jit\n","def accuracy_step(params, images, labels):\n","    \"\"\"\n","    Computes accuracy for a single batch.\n","    \"\"\"\n","    logits = model.apply({'params': params}, images)\n","    # Get the index of the highest probability.\n","    predictions = jnp.argmax(logits, axis=-1)\n","    # Labels are one-hot, so we need the index there too.\n","    actual = jnp.argmax(labels, axis=-1)\n","    return jnp.mean(predictions == actual)\n","\n","def evaluate_model(params, dataset, num_batches=10):\n","    \"\"\"\n","    Runs the model over several test batches to estimate accuracy.\n","    \"\"\"\n","    accs = []\n","    for _ in range(num_batches):\n","        batch_images, batch_labels = next(dataset)\n","        acc = accuracy_step(params, batch_images, batch_labels)\n","        accs.append(acc)\n","\n","    return np.mean(accs)\n","\n","# Check performance on the test set.\n","test_acc = evaluate_model(params, test_ds, num_batches=20)\n","print(f\"Test Accuracy after 500 steps: {test_acc:.2%}\")"],"metadata":{"cellView":"form","id":"T4Ur_CxUx5BU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Parallel training logic\n","from functools import partial\n","\n","# Initial replication of state across available silicon.\n","# Even for a single T4, using pmap now saves a refactor later when scaling.\n","replicated_params = jax.device_put_replicated(params, jax.local_devices())\n","replicated_opt_state = jax.device_put_replicated(opt_state, jax.local_devices())\n","\n","@partial(jax.pmap, axis_name='batch')\n","def parallel_train_step(params, opt_state, images, labels):\n","    \"\"\"\n","    Standard data-parallel update with pmean for gradient sync.\n","    \"\"\"\n","    loss, grads = jax.value_and_grad(compute_loss)(params, images, labels)\n","\n","    # Collective reduction across the specified axis.\n","    grads = jax.lax.pmean(grads, axis_name='batch')\n","    loss = jax.lax.pmean(loss, axis_name='batch')\n","\n","    updates, opt_state = optimizer.update(grads, opt_state, params)\n","    params = optax.apply_updates(params, updates)\n","\n","    return params, opt_state, loss\n","\n","def reshape_for_pmap(images, labels):\n","    \"\"\"\n","    Manual sharding: splits global batch into [num_devices, local_batch, ...].\n","    \"\"\"\n","    n_dev = jax.local_device_count()\n","    if images.shape[0] % n_dev != 0:\n","        raise ValueError(f\"Batch size {images.shape[0]} not divisible by {n_dev} devices.\")\n","\n","    img_reshaped = images.reshape((n_dev, -1) + images.shape[1:])\n","    lbl_reshaped = labels.reshape((n_dev, -1) + labels.shape[1:])\n","    return img_reshaped, lbl_reshaped\n","\n","# Smoke test for the parallel path.\n","batch_images, batch_labels = next(train_ds)\n","p_images, p_labels = reshape_for_pmap(batch_images, batch_labels)\n","\n","replicated_params, replicated_opt_state, p_loss = parallel_train_step(\n","    replicated_params, replicated_opt_state, p_images, p_labels\n",")\n","\n","# Extract first device's loss for verification.\n","print(f\"Parallel step loss: {p_loss[0]:.4f}\")"],"metadata":{"cellView":"form","id":"2TcozpbPzE3s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Distributed training loop\n","import time\n","\n","# Move parameters and optimizer state to accelerator memory.\n","# This ensures each device has its own local copy for the pmap operations.\n","replicated_params = jax.device_put_replicated(params, jax.local_devices())\n","replicated_opt_state = jax.device_put_replicated(opt_state, jax.local_devices())\n","\n","def train_parallel(params, opt_state, dataset, num_steps=1000):\n","    \"\"\"\n","    Executes parallel training across the device topology.\n","    \"\"\"\n","    start_time = time.time()\n","    print(f\"Executing parallel training for {num_steps} steps...\")\n","\n","    for step in range(num_steps):\n","        # Pull next global batch from host pipeline.\n","        batch_images, batch_labels = next(dataset)\n","\n","        # Partition global batch into device-local shards.\n","        p_images, p_labels = reshape_for_pmap(batch_images, batch_labels)\n","\n","        # Synchronized update across devices.\n","        params, opt_state, loss = parallel_train_step(\n","            params, opt_state, p_images, p_labels\n","        )\n","\n","        if step % 200 == 0:\n","            # Report scalar loss from the lead device.\n","            print(f\"Step {step:4d} | Multi-device Loss: {loss[0]:.4f}\")\n","\n","    total_time = time.time() - start_time\n","    print(f\"\\nParallel training complete.\")\n","    print(f\"Throughput: {num_steps/total_time:.2f} steps/s\")\n","\n","    return params, opt_state\n","\n","# Run the optimized loop.\n","replicated_params, replicated_opt_state = train_parallel(\n","    replicated_params, replicated_opt_state, train_ds\n",")"],"metadata":{"cellView":"form","id":"phAO8hEQzWd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Final evaluation\n","\n","# Strip the device dimension to get back to a single-device parameter set.\n","final_params = jax.tree_util.tree_map(lambda x: x[0], replicated_params)\n","\n","def evaluate_final(params, dataset, num_batches=50):\n","    \"\"\"\n","    Final accuracy check on the test set.\n","    \"\"\"\n","    accuracies = []\n","    for _ in range(num_batches):\n","        batch_images, batch_labels = next(dataset)\n","        # Use the compiled accuracy function.\n","        acc = accuracy_step(params, batch_images, batch_labels)\n","        accuracies.append(acc)\n","    return np.mean(accuracies)\n","\n","final_accuracy = evaluate_final(final_params, test_ds)\n","print(f\"Final Test Accuracy: {final_accuracy:.2%}\")"],"metadata":{"cellView":"form","id":"y-UXh5M-090E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Prediction visualization\n","\n","import matplotlib.pyplot as plt\n","\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","@jax.jit\n","def get_predictions(params, images):\n","    logits = model.apply({'params': params}, images)\n","    return jnp.argmax(logits, axis=-1)\n","\n","def plot_predictions(params, images, labels, num_samples=10):\n","    # Process the whole slice at once.\n","    preds = get_predictions(params, images[:num_samples])\n","    actuals = jnp.argmax(labels[:num_samples], axis=-1)\n","\n","    plt.figure(figsize=(15, 5))\n","    for i in range(num_samples):\n","        plt.subplot(2, 5, i + 1)\n","        plt.imshow(images[i])\n","\n","        color = 'green' if preds[i] == actuals[i] else 'red'\n","        title = f\"P: {class_names[preds[i]]}\\nA: {class_names[actuals[i]]}\"\n","        plt.title(title, color=color, fontsize=10)\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Grab a batch and plot.\n","vis_images, vis_labels = next(test_ds)\n","plot_predictions(final_params, vis_images, vis_labels)"],"metadata":{"cellView":"form","id":"CtFuMhRm1PX4"},"execution_count":null,"outputs":[]}]}